{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZaSGO7y3nzOVyO6+MyI2I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# House Project 모델 정리\n","\n","## 트리기반분류모델 (Tree-Based Classification Model)\n","트리 기반 분류 모델은 데이터 분류 작업에 사용되는 기본적인 머신러닝 알고리즘입니다. 이 모델은 데이터 세트를 반복적으로 이진 트리 구조로 나누는 방식으로 작동합니다. 각 노드는 특정 속성에 따라 데이터를 분류하며, 최종 노드(리프 노드)는 클래스 레이블을 결정합니다.\n","\n","- **데이터 분할을 통해 예측 모델 생성**: 트리 기반 모델은 데이터를 여러 번 분할하여 최종 예측 모델을 만듭니다.\n","- **이해 및 시각화가 쉬움**: 모델이 결정 트리 형태로 표현되기 때문에 직관적으로 이해하고 시각화하기가 쉽습니다.\n","- **과적합 문제 발생 가능**: 데이터에 과적합되는 경향이 있어, 신중한 가지치기(pruning)가 필요합니다.\n","\n","<br/><br/>\n","\n","## 랜덤 포레스트 모델 (Random Forest Model)\n","랜덤 포레스트는 여러 개의 결정 트리(decision tree)를 사용하여 예측을 수행하는 앙상블 학습(ensemble learning) 기법입니다. 각 트리는 훈련 데이터의 다른 부분에서 학습되며, 최종 예측은 모든 트리의 예측을 평균하여 결정됩니다.\n","\n","- **높은 예측 정확도**: 여러 트리의 예측을 평균화하여 최종 예측을 수행하므로 높은 정확도를 보입니다.\n","- **과적합 방지**: 여러 트리를 사용하여 예측을 수행하기 때문에 단일 트리보다 과적합의 위험이 줄어듭니다.\n","- **중요 변수 선택 가능**: 각 변수의 중요도를 평가할 수 있습니다.\n","- **트리의 수와 깊이에 따라 성능 결정**: 사용되는 트리의 수와 깊이가 모델의 성능에 큰 영향을 미칩니다.\n","\n","<br/>\n","\n","### 랜덤 포레스트 모델을 이해하기 위한 선행 지식\n","랜덤 포레스트를 이해하기 위해서는 결정 트리의 기본 원리와 앙상블 학습 기법에 대한 이해가 필요합니다. 랜덤 포레스트는 다수의 약한 학습기(weak learner)를 결합하여 강력한 학습기(strong learner)를 만드는 방식입니다.\n","\n","<br/><br/>\n","\n","## 거리기반분류모델 (Distance-Based Classification Model)\n","거리 기반 분류 모델은 데이터 포인트 간의 거리를 측정하여 분류를 수행하는 모델입니다. 대표적인 예로 k-최근접 이웃 알고리즘(k-Nearest Neighbors, k-NN)이 있습니다. k-NN은 새로운 데이터 포인트의 클래스를 결정하기 위해 가장 가까운 k개의 이웃 데이터 포인트를 고려합니다.\n","\n","- **간단하고 직관적인 모델**: k-NN 알고리즘은 이해하기 쉽고 구현이 간단합니다.\n","- **계산 비용이 높을 수 있음**: 데이터 포인트 간의 거리를 계산해야 하므로 큰 데이터셋에서는 계산 비용이 높을 수 있습니다.\n","- **데이터의 규모와 차원에 민감**: 데이터의 규모가 커지거나 차원이 높아질수록 성능이 저하될 수 있습니다.\n","- **거리 측정 방식에 따라 성능 달라짐**: 유클리드 거리, 맨하탄 거리 등 다양한 거리 측정 방식을 사용할 수 있으며, 선택한 방식에 따라 모델 성능이 달라질 수 있습니다.\n","\n","<br/><br/>\n","\n","\n","## 회귀 모델 (Regression Model)\n","회귀 모델은 연속적인 값을 예측하기 위해 사용되는 통계적 기법입니다. 회귀 분석은 독립 변수와 종속 변수 간의 관계를 모델링하여 예측을 수행합니다.\n","\n","### 주요 유형\n","- **선형 회귀 (Linear Regression)**: 독립 변수와 종속 변수 간의 선형 관계를 모델링합니다.\n","- **로지스틱 회귀 (Logistic Regression)**: 종속 변수가 범주형인 경우 사용되며, 결과는 확률로 표현됩니다.\n","\n","<br/><br/>\n","\n","## 라쏘 모델 (Lasso Model)\n","라쏘(Lasso, Least Absolute Shrinkage and Selection Operator) 모델은 선형 회귀 모델의 일종으로, L1 정규화를 적용하여 모델의 복잡도를 제어하고 중요하지 않은 변수의 계수를 0으로 만들어 변수 선택을 자동으로 수행합니다.\n","\n","### 주요 특징\n","- **모델의 복잡도 제어**: L1 정규화를 통해 모델의 복잡도를 제어합니다.\n","- **변수 선택 기능**: 중요하지 않은 변수의 계수를 0으로 만들어 자동으로 변수 선택을 수행합니다.\n","\n","<br/><br/>\n","\n","## 릿지 모델 (Ridge Model)\n","릿지(Ridge) 회귀 모델은 선형 회귀 모델에 L2 정규화를 적용하여 과적합을 방지하는 모델입니다. 모든 변수의 계수를 작게 만들어 모델의 복잡도를 줄이는 방식입니다.\n","\n","### 주요 특징\n","- **과적합 방지**: L2 정규화를 통해 모델의 복잡도를 줄여 과적합을 방지합니다.\n","- **모든 변수 사용**: 변수의 계수를 작게 만들지만, 모든 변수를 사용합니다.\n","\n","\n","<br/><br/>\n","\n","## Elastic Net 모델 (Elastic Net Model)\n","Elastic Net 모델은 라쏘와 릿지 회귀의 장점을 결합한 모델로, L1과 L2 정규화를 동시에 사용합니다. 이 모델은 변수 선택과 과적합 방지를 모두 수행합니다.\n","\n","### 주요 특징\n","- **혼합 정규화**: L1과 L2 정규화를 동시에 사용하여 모델의 성능을 향상시킵니다.\n","- **변수 선택과 과적합 방지**: 라쏘의 변수 선택 기능과 릿지의 과적합 방지 기능을 모두 포함합니다.\n","\n","<br/><br/>\n","\n","> 타깃변수가 이진값 변수이면 RandomForestRegressor, 타깃변수가 연속형 변수이면 RandomForestClassifier"],"metadata":{"id":"xcAK5EUrdL0_"}},{"cell_type":"markdown","source":["# L1 정규화 (L1 Regularization)\n","\n","## 정의\n","L1 정규화는 머신러닝 모델에서 과적합을 방지하기 위해 가중치의 절대값 합을 최소화하는 기법입니다. 주로 라쏘 회귀(Lasso Regression)에서 사용됩니다.\n","\n","## 수식\n","L1 정규화는 다음과 같은 형태의 비용 함수에 추가됩니다: <br/>\n","#### Cost Function = Loss + λ ∑|β_j|\n","\n","여기서 λ 는 정규화 강도를 조절하는 하이퍼파라미터이고, β는 모델의 가중치입니다.\n","\n","## 주요 특징\n","- **가중치 희소성**: L1 정규화는 중요하지 않은 변수의 가중치를 0으로 만들어 변수 선택을 자동으로 수행합니다.\n","- **해석 용이성**: 결과 모델이 희소성을 가지므로, 해석이 용이합니다.\n","- **변수 선택**: 불필요한 변수를 제거하여 모델의 성능을 향상시킵니다.\n","\n","## 장점\n","- 불필요한 변수를 제거하여 모델을 단순화합니다.\n","- 과적합을 방지하여 모델의 일반화 성능을 향상시킵니다.\n","\n","## 단점\n","- 데이터가 매우 큰 경우에는 계산 비용이 높아질 수 있습니다.\n","- 모든 변수의 중요성을 동일하게 고려하지 않기 때문에 중요한 변수가 제거될 가능성도 있습니다.\n","\n","<br/><br/><br/>\n","\n","# L2 정규화 (L2 Regularization)\n","\n","## 정의\n","L2 정규화는 머신러닝 모델에서 과적합을 방지하기 위해 가중치의 제곱 합을 최소화하는 기법입니다. 주로 릿지 회귀(Ridge Regression)에서 사용됩니다.\n","\n","## 수식\n","L2 정규화는 다음과 같은 형태의 비용 함수에 추가됩니다: <br/>\n"," Loss + λ ∑(β_j^2)\n","여기서 λ는 정규화 강도를 조절하는 하이퍼파라미터이고, β는 모델의 가중치입니다.\n","\n","## 주요 특징\n","- **가중치 축소**: 모든 변수의 가중치를 작게 만들어 과적합을 방지합니다.\n","- **매끄러운 해결**: L2 정규화는 가중치가 0이 되지 않도록 하여 모든 변수를 모델에 포함시킵니다.\n","- **안정성**: 높은 다중공선성을 가진 데이터셋에 적합합니다.\n","\n","## 장점\n","- 모든 변수를 포함시켜 예측 모델의 안정성을 높입니다.\n","- 다중공선성이 있는 데이터셋에 잘 작동합니다.\n","- 과적합을 방지하여 모델의 일반화 성능을 향상시킵니다.\n","\n","## 단점\n","- 모든 변수가 모델에 포함되므로 해석이 어려울 수 있습니다.\n","- 데이터가 희소할 경우 성능이 저하될 수 있습니다.\n","\n","## 결론\n","L1과 L2 정규화는 모두 모델의 과적합을 방지하는 데 유용한 기법입니다. L1 정규화는 변수 선택 기능을 제공하며, L2 정규화는 모든 변수를 포함시켜 모델의 안정성을 높입니다. 두 방법의 적절한 선택은 데이터의 특성과 분석 목적에 따라 달라집니다."],"metadata":{"id":"42kAD2LhebRn"}},{"cell_type":"code","source":[],"metadata":{"id":"8qXyeWQre_yc"},"execution_count":null,"outputs":[]}]}